---
title: "Data Challenge 2 Leaderboard (Probabilistic short-term forecasting of global ocean dynamics)"
---

```{python}
import json
import numpy as np
import pandas as pd
from pathlib import Path
import re
from IPython.display import display, Markdown

metrics_names = {
    "rmse": "Root Mean Squared Error",
    "rmsd": "Root Mean Squared Deviation",
    "rmsd_geostrophic_currents": "RMSD of Geostrophic Currents",
    "rmsd_mld": "RMSD of Mixed Layer Depth",
    "lagrangian": "Lagrangian analysis",
}

# Fonction pour extraire la profondeur d'une variable
def get_depth_order(variable_name):
    variable_lower = variable_name.lower()
    if 'surface' in variable_lower:
        return 0
    # Extraire les nombres pour les profondeurs (50m, 200m, etc.)
    depth_match = re.search(r'(\d+)m', variable_lower)
    if depth_match:
        return int(depth_match.group(1))
    return 999  # Mettre les variables sans profondeur à la fin

# Fonction générique pour extraire le type de variable (sans la profondeur)
def get_variable_type(variable_name):
    variable_lower = variable_name.lower()
    
    # Enlever les indications de profondeur
    # Supprimer "surface", les profondeurs en mètres, etc.
    cleaned_var = re.sub(r'\bsurface\b', '', variable_lower)
    cleaned_var = re.sub(r'\d+m\b', '', cleaned_var)
    cleaned_var = re.sub(r'\d+_m\b', '', cleaned_var)
    cleaned_var = re.sub(r'_surface\b', '', cleaned_var)
    
    # Nettoyer les underscores et espaces multiples
    cleaned_var = re.sub(r'_+', '_', cleaned_var)
    cleaned_var = cleaned_var.strip('_').strip()
    
    # Si la variable nettoyée est vide, utiliser la variable originale
    if not cleaned_var:
        return variable_name.lower()
    
    return cleaned_var

# Fonction pour trier les variables par type puis par profondeur
def sort_variables_by_type_and_depth(variables):
    def sort_key(var):
        var_type = get_variable_type(var)
        depth = get_depth_order(var)
        return (var_type, depth, var)
    return sorted(variables, key=sort_key)

# 1. Lecture de tous les fichiers JSON du dossier results/
files = list(Path("results").glob("*.json"))
data = []
models = {}
reference_model = "glonet"

for file in files:
    with open(file) as f:
        content = json.load(f)
        # Extraire le modèle de référence depuis le champ "dataset"
        if isinstance(content, dict) and "dataset" in content:
            models[file.name] = content["dataset"]
            continue


for file in files:
    with open(file) as f:
        content = json.load(f)
        
        # Vérifier que c'est le bon format
        if "dataset" in content and "results" in content:
            dataset_name = content["dataset"]
            
            # Parcourir les résultats pour ce dataset
            for model_key, entries in content["results"].items():
                if isinstance(entries, list):
                    for entry in entries:
                        model = entry.get("model", model_key)
                        ref_alias = entry.get("ref_alias", "unknown")
                        lead_time = entry.get("lead_time", None)
                        lead_day = f"Lead day {lead_time + 1}" if lead_time is not None else "unknown"
                        result = entry.get("result", [])
                        
                        # Traiter chaque item dans result
                        if isinstance(result, list):
                            for item in result:
                                metric = item.get("Metric", "unknown")
                                variable = item.get("Variable", "unknown")
                                value = item.get("Value", 0)
                                
                                data.append({
                                    "model": model,
                                    "metric": metric,
                                    "lead_day": lead_day,
                                    "variable": variable,
                                    "score": value,
                                    "ref_alias": ref_alias,
                                    "dataset": dataset_name
                                })
                        elif isinstance(result, dict):
                            # Traiter le format dict (comme pour glorys)
                            for metric_name, variables in result.items():
                                for variable, score in variables.items():
                                    data.append({
                                        "model": model,
                                        "metric": metric_name,
                                        "lead_day": lead_day,
                                        "variable": variable,
                                        "score": score,
                                        "ref_alias": ref_alias,
                                        "dataset": dataset_name
                                    })


df = pd.DataFrame(data)

# Debug pour vérifier les colonnes
'''print("Colonnes du DataFrame:", df.columns.tolist())
print("Nombre de lignes:", len(df))
if len(df) > 0:
    print("Premières lignes:")
    print(df.head())'''

# cmap_code = 'RdYlGn_r'
# cmap_code = 'RdBu_r'
cmap_code = "coolwarm"
# cmap_code = "seismic"

vars_group_size = 3  # tableaux contenant vars_group_size variables


# 1. Sélectionner les lead days en priorité au lead day 1
def get_lead_days_for_display(all_lead_days, max_count=4):
    """
    Sélectionne les lead days pour l'affichage :
    - Toujours inclure Lead day 1 s'il existe
    - Prendre les lead days impairs
    - Limiter le nombre total et supprimer le dernier si nécessaire
    """
    # Extraire et trier tous les lead days numériquement
    lead_days_with_nums = []
    for ld in all_lead_days:
        m = re.search(r'(\d+)', ld)
        if m:
            lead_days_with_nums.append((int(m.group(1)), ld))
    
    lead_days_with_nums.sort()
    
    # Sélectionner lead day 1 en priorité, puis les impairs
    selected = []
    
    # Ajouter Lead day 1 s'il existe
    for num, ld in lead_days_with_nums:
        if num == 1:
            selected.append(ld)
            break
    
    # Ajouter les autres lead days impairs (sauf 1 déjà ajouté)
    for num, ld in lead_days_with_nums:
        if num % 2 == 1 and num != 1 and len(selected) < max_count:
            selected.append(ld)
    
    # Si on a trop de lead days, supprimer le dernier (le plus grand)
    if len(selected) > max_count:
        selected = selected[:-1]
    
    return selected

# Vérifier que df contient bien 'lead_day'
if 'lead_day' in df.columns:
    all_unique_lead_days = df['lead_day'].unique()
    lead_days = get_lead_days_for_display(all_unique_lead_days, max_count=4)
    # print(f"Lead days sélectionnés pour l'affichage : {lead_days}")
else:
    # print("Attention: colonne 'lead_day' manquante")
    lead_days = []





def bold_reference_index(val, reference_model):
    return 'font-weight: bold;' if val == reference_model else ''

def chunks(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

display(Markdown('<div style="height: 90px;"></div>'))

# 2. Pour chaque dataset de référence et chaque métrique, construire le tableau
if len(df) > 0:
    for ref_alias in sorted(df['ref_alias'].unique()):
        display(Markdown(f"## Reference dataset: {ref_alias.upper()}"))
        
        ref_df = df[df.ref_alias == ref_alias]
        
        # Déterminer le modèle de référence pour ce dataset
        datasets_in_ref = ref_df['dataset'].unique()
        if "glonet" in datasets_in_ref:
            reference_model = "glonet"
        else:
            # Prendre le premier dataset comme référence
            reference_model = datasets_in_ref[0]
        
        # Boucle sur les métriques pour ce dataset de référence
        for metric in sorted(ref_df.metric.unique()):
            metric_complete_name = metrics_names.get(metric, metric)
            display(Markdown(f"### Metric: {metric_complete_name}"))
            
            # Filtrer sur le ref_alias ET la métrique
            ref_metric_df = ref_df[ref_df.metric == metric]
            
            # Trier les variables par type et profondeur
            metric_variables = sort_variables_by_type_and_depth(ref_metric_df['variable'].unique())

            # Grouper les variables par type
            variables_by_type = {}
            for var in metric_variables:
                var_type = get_variable_type(var)
                if var_type not in variables_by_type:
                    variables_by_type[var_type] = []
                variables_by_type[var_type].append(var)

            # Traiter chaque groupe de variables
            for var_type, var_group in variables_by_type.items():
                display(Markdown(f"#### {var_type.title()} Variables"))
                # display(Markdown(f"#### {var_type.title()} Variables: {', '.join(var_group)}"))
                
                # Filtrer sur variables du groupe et lead days impairs
                sub = ref_metric_df[ref_metric_df.variable.isin(var_group) & ref_metric_df.lead_day.isin(lead_days)]
                
                if len(sub) == 0:
                    display(Markdown("*Aucune donnée disponible pour ces variables et lead days.*"))
                    continue
                
                # S'assurer que lead day 1 est inclus s'il existe
                available_leads = sorted(sub['lead_day'].unique(), key=lambda x: int(re.search(r'\d+', x).group(0)))

                # Utiliser les lead days pré-sélectionnés qui sont disponibles dans les données
                common_leads = [ld for ld in lead_days if ld in available_leads]

                # Si aucun lead day commun, prendre les premiers disponibles
                if not common_leads:
                    common_leads = available_leads[:4]

                sub = sub[sub.lead_day.isin(common_leads)]
                
                if len(sub) == 0:
                    display(Markdown("*Pas de données disponibles pour les lead days sélectionnés.*"))
                    continue
                
                # Pivot multi-index avec ordre correct des variables
                # D'abord trier les variables dans l'ordre voulu
                ordered_variables = [var for var in var_group if var in sub['variable'].unique()]
                
                pivot = sub.pivot_table(index="model", columns=["variable", "lead_day"], values="score", aggfunc="mean")
                
                # Réorganiser les colonnes pour avoir l'ordre correct
                if isinstance(pivot.columns, pd.MultiIndex):
                    # Créer un nouvel ordre de colonnes
                    new_columns = []
                    for var in ordered_variables:
                        for lead in common_leads:
                            if (var, lead) in pivot.columns:
                                new_columns.append((var, lead))
                    
                    # Réindexer avec le nouvel ordre
                    pivot = pivot.reindex(columns=new_columns)

                if len(pivot) == 0:
                    display(Markdown("*Pas de données à afficher.*"))
                    continue

                # Réordonner pour mettre le modèle de référence en premier
                if reference_model in pivot.index:
                    new_order = [reference_model] + [m for m in pivot.index if m != reference_model]
                    pivot = pivot.reindex(new_order)
                    ref_values = pivot.loc[reference_model]
                    percent_diff = (pivot - ref_values) / ref_values * 100
                else:
                    # Si pas de modèle de référence, pas de coloration
                    ref_values = None
                    percent_diff = pd.DataFrame(0, index=pivot.index, columns=pivot.columns)

                # Calcul dynamique de l'échelle de couleur
                if ref_values is not None:
                    non_ref_values = percent_diff.values[1:] if len(percent_diff) > 1 else []
                    if len(non_ref_values) > 0 and not np.all(np.isnan(non_ref_values)):
                        absmax = np.nanmax(np.abs(non_ref_values))
                        absmax = max(absmax, 2)  # minimum % pour un contraste plus fort
                    else:
                        absmax = 2  # valeur par défaut
                else:
                    absmax = 2
                
                import matplotlib.pyplot as plt
                norm = plt.Normalize(-absmax, absmax)

                def color_cells(val, percent):
                    from matplotlib import cm
                    cmap = plt.get_cmap(cmap_code)
                    if pd.isna(percent):
                        return ''
                    color = cmap(norm(percent))
                    return f'background-color: rgba({int(color[0]*255)},{int(color[1]*255)},{int(color[2]*255)},{color[3]:.2f})'

                def style_func(df):
                    styled = pd.DataFrame('', index=df.index, columns=df.columns)
                    for i in df.index:
                        for j in df.columns:
                            percent = percent_diff.loc[i, j]
                            styled.loc[i, j] = color_cells(df.loc[i, j], percent)
                    return styled

                # Déterminer où commence chaque variable pour les bordures
                styles = []
                if isinstance(pivot.columns, pd.MultiIndex):
                    prev_var = None
                    col_idx = 0
                    for var, lead in pivot.columns:
                        if var != prev_var and prev_var is not None:
                            # Ajouter une bordure au début de chaque nouvelle variable
                            styles.append({
                                'selector': f'th.col{col_idx}, td.col{col_idx}',
                                'props': [('border-left', '3px solid #333')]
                            })
                        prev_var = var
                        col_idx += 1

                styled = pivot.style.apply(style_func, axis=None).format("{:.3f}")
                pivot.columns.names = ['Variable', 'Lead Day']
                pivot.index.name = None

                styled = styled.set_table_styles(styles, axis=1)
                
                # Styles supplémentaires
                extra_styles = [
                    {'selector': 'td:not(:first-child)', 'props': [('text-align', 'center')]},
                    {'selector': 'th.col_heading', 'props': [('text-align', 'center')]},
                    {'selector': 'th.col_heading.level0', 'props': [('text-align', 'center'), ('font-weight', 'bold')]},
                    {'selector': 'th.col_heading.level1', 'props': [('text-align', 'center'), ('font-size', '0.9em')]}
                ]
                styled = styled.set_table_styles(styles + extra_styles, axis=1)
                styled = styled.map_index(lambda val: bold_reference_index(val, reference_model), axis=0)
                display(styled)

                display(Markdown('<div style="height: 50px;"></div>'))
else:
    display(Markdown("## Aucune donnée trouvée dans les fichiers JSON"))
```

---

<div style="height: 50px;"></div>

<div style="display: flex; justify-content: center;">
```{python}
import matplotlib.pyplot as plt
from matplotlib import cm

cmap_code = "coolwarm"
cmap = plt.get_cmap(cmap_code)
norm = plt.Normalize(-100, 100)

fig, ax = plt.subplots(figsize=(8, 1.2))
fig.subplots_adjust(left=0.18, right=0.98, bottom=0.5, top=0.8)

cb1 = plt.colorbar(
    cm.ScalarMappable(norm=norm, cmap=cmap),
    cax=ax, orientation='horizontal'
)
cb1.set_label('% difference in metric compared to reference model', fontsize=10)
cb1.set_ticks([-100, -50, 0, 50, 100])
cb1.set_ticklabels(['-100%', '-50%', '0%', '+50%', '+100%'])
plt.show()
```
</div>